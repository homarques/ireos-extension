%!TEX root = ../sample-acmlarge.tex
Let $\mathbf{X} = \{\mathbf{x}_1, \cdots, \mathbf{x}_N \}$ be an unlabeled data set containing $N$ $d$-dimensional feature vectors, $\mathbf{x}_i$, and assume that one or more unsupervised outlier detection algorithms will produce, for this data set, candidate outliers solutions, which one wants to evaluate in the absence of labels. Solutions produced by unsupervised outlier detection algorithms can be given in different formats. The most common format is given as a set $\mathbf{y} = \{ y_1, \cdots, y_N \}$, $y_i \in \mathbb{R}^+$, where $y_i$ represents the outlier score associated with the observation $\mathbf{x}_i$, that reflects the degree of outlierness of $\mathbf{x}_i$. Such solutions allow that the observations $\mathbf{x}_i$ be sorted and ranked according to their degree of outlierness $y_i$. When the number of outliers $n$ is known, one can establish a threshold in the ranking in order to select a subset $\mathbf{S} \subset \mathbf{X}$, $|\mathbf{S}| = n$, containing the top-$n$ observations that are labeled as outliers. Given a collection of candidate solutions $\mathbf{S}$, usually called of top-$n$ (binary) outlier detection solution, IREOS \cite{marques2015} is able to independently and quantitatively measure the quality of each individual candidate solution in the absence of labels.

The criteria used by IREOS is based on the common intuition that an outlier is an observation that is to some extent farther off and can therefore be more easily separated (discriminated) from other observations than an inlier. In order to quantify how easily an observation can be separated from the other observations, IREOS uses a maximum margin classifier \cite{bishop2006,hastie2013} to assess the separability of individual observations, using the distance from the observation to the decision boundary as the base of the separability measure used in the evaluation. However, the separation of certain observations (such as those within a cluster) implies in the use of nonlinear maximum margin classifiers, such as Support Vector Machine (SVM) or Kernel Logistic Regression (KLR) \cite{bishop2006,hastie2013}, as these observations require nonlinear decision boundary to be separated. These classifiers use a kernel function, such as the radial basis kernel \cite{scholkopf2001}, to transform the original (possibly non-linearly separable) problem into a linearly separable one. The use of a kernel function demands the configuration of the kernel parameter $\gamma$. According to the discussion in \citep{marques2015}, the choice of a single parameter value is not necessary for this problem, instead, a range of value of $\gamma$ from 0 up to a maximum value $\gamma_{max}$ (for which all the observations labeled as outliers can be individually discriminated from all the others by using a kernel-based classifier) is chosen and the \textit{area under the curve} (AUC) formed by the overall separability of an observation $\mathbf{x}_i$ over the interval of $\gamma$ values (Fig. \ref{fig:ind_avg_curv}) is taken,

\begin{equation}
\int_{\gamma = 0}^{\gamma_{max}} p(\mathbf{x}_i, \gamma)
\end{equation}
where $p(\mathbf{x}_i, \gamma)$ quantifies in a normalized interval how far each observation $\mathbf{x}_i$ is from the decision boundary.

In order to evaluate the quality of a given solution $\mathbf{S}$, the average curve of separability for those observations in $\mathbf{S}$ is taken, 

\begin{equation}
\int_{\gamma = 0}^{\gamma_{max}} \bar{p}(\gamma) = \int_{\gamma = 0}^{\gamma_{max}} \frac{1}{n} \sum_{\mathbf{x}_i \in \mathbf{S}} p(\mathbf{x}_i, \gamma)
\label{eq:ireos:avg_curve}
\end{equation}

As in practice classifiers need to be trained to compute $\bar{p}(\gamma)$ for each $\gamma$, the interval $[0, \gamma_{max}]$ is discretized into a finite number of values for $\gamma$, from $\gamma_1 = 0$ to $\gamma_{n_\gamma} = \gamma_{max}$, and the index can thus be computed (within $[0, 1]$) as:

\begin{equation}
I(\mathbf{S}) = \frac{1}{n_{\gamma}} \sum_{l = 1}^{n_{\gamma}} \left( \frac{1}{n} \sum_{\mathbf{x}_i \in \mathbf{S}} p(\mathbf{x}_i, \gamma_l) \right)
\label{eq:original_ireos}
\end{equation}

\subsection{Modeling clumps}

Clumps, or particles, are subsets of observations lying in the same region of the data space, relatively close to each other than they are from other observations, but too small to be deemed a cluster. In order to modeling the presence of possible clumps, IREOS proposes the use of soft margin classifiers with individual penalties for each observation \cite{osuna1997}. These classifiers allow the misclassification of observations at the price of a penalty term $P_t$ (the cost of the misclassification can be different for each observation) that is incorporated into the original objective of margin maximization. Such a term is typically in the form
\begin{equation}
P_t = \sum_{i = 1}^{N}C(\mathbf{x}_i)\xi(\mathbf{x}_i), 
\end{equation}
where $\xi(\mathbf{x}_i)$ stands for the individual penalty component associated with observation $\mathbf{x}_i$. The farther an observation $\mathbf{x}_i$ is from the margin boundary on the wrong side, the greater the value of $\xi(\mathbf{x}_i)$. $C(\mathbf{x}_i)$ controls the cost of penalties of the observations $\mathbf{x}_i$, where the full cost $C$ is assigned to the observations labeled as inliers ($C(\mathbf{x}_i) = C$) yet only a fraction $\beta \in [0, 1]$ of $C$ to the observations labeled as outliers ($C(\mathbf{x}_i) = \beta \cdot C$). The setting of $\beta$ controls the effect that the labeling of the other observations will have when assessed the separability of a particular observation. For $\beta = 1$, the method reduces to the ordinary case where observations are treated equally no matter their labels. In the other extreme, $\beta = 0$, observations labeled as outliers can be misclassified for free (notice that, when evaluating the separability of a specific observation, this is equivalent to removing all other observations labeled as outliers from the data set).

The fraction of the penalty ($\beta$) is set from an optional control parameter introduced by IREOS, the maximum clump size ($m_{cl}$), that allow the users adjust their expectations about clump sizes. The optional control parameter is set as $\beta = \frac{1}{m_{cl}}$. The control parameter is optional because by setting $m_{cl} = 1$, the method reduces to the particular case where clumps are not modeled and the same, full penalty cost is assigned to all observations. By setting $\beta = \frac{1}{m_{cl}}$, one needs $m_{cl}$ observations (a full clump) labeled as outliers to get the same impact as a single inlier. Notice that except when $m_{cl} = 1$, the separability of each observation in the general case depends on the labels of the other observations.

%The maximum clump size, $m_{cl}$, is a mechanism that allows different users in varied application scenarios to explicitly express what they judge ``too small'' to be interpreted as a cluster.
%The possible presence of clumps in the data set induces IREOS to provide the users with an optional control mechanism to adjust their expectations about clump sizes. 